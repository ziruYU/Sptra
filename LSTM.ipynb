{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "indice1=300\n",
    "sample=32\n",
    "cl=16\n",
    "indice1 = 300\n",
    "\n",
    "train = np.load(\"train.npy\")\n",
    "train = torch.from_numpy(np.stack(train, 0))\n",
    "test = np.load(\"test.npy\")\n",
    "test = torch.from_numpy(np.stack(test, 0))\n",
    "valid = np.load(\"valid.npy\")\n",
    "valid = torch.from_numpy(np.stack(valid, 0))\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "label=np.arange(0,cl,1)\n",
    "label = label.astype(np.float32)\n",
    "train_label = torch.from_numpy(np.tile(label, indice1)) \n",
    "test_label  = torch.from_numpy(np.tile(label, 100))\n",
    "valid_label = torch.from_numpy(np.tile(label, 2))\n",
    "\n",
    "print(train_label.dtype)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "print(valid_label.shape)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_dataset = TensorDataset(train, train_label)\n",
    "valid_dataset = TensorDataset(valid, valid_label)\n",
    "test_dataset = TensorDataset(test, test_label)\n",
    "batch=cl*2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch,            \n",
    "    shuffle= True,          \n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = batch,           \n",
    "    shuffle= True,          \n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch,     \n",
    "    shuffle= True,            \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4800, 32, 204])\n",
      "torch.Size([1600, 32, 204])\n",
      "torch.Size([32, 32, 204])\n",
      "torch.float32\n",
      "torch.Size([4800])\n",
      "torch.Size([1600])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Sequence Modeling')\n",
    "parser.add_argument('--batch_size', type=int, default=cl*2, metavar='N',\n",
    "                    help='batch size (default: 128)')\n",
    "parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                    help='Disable cuda for training')\n",
    "parser.add_argument('--dropout', type=float, default=0.45,\n",
    "                    help='dropout applied to layers (default: 0.05)')\n",
    "parser.add_argument('--clip', type=float, default=0.35,\n",
    "                    help='gradient clip, -1 means no clip (default: -1)')\n",
    "parser.add_argument('--epochs', type=int, default=30,\n",
    "                    help='upper epoch limit (default: 20)')\n",
    "parser.add_argument('--levels', type=int, default=4,\n",
    "                    help='# of levels (default: 8)')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                    help='report interval (default: 100')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='initial learning rate (default: 2e-3)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed (default: 1111)')\n",
    "parser.add_argument('--seq_len', type=int, default=204)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "print(args)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "batch_size = args.batch_size  \n",
    "print(batch_size)\n",
    "epochs = args.epochs \n",
    "input_channels = sample\n",
    "print(input_channels)\n",
    "channel_sizes = [input_channels*2] * args.levels   \n",
    "n_classes = cl\n",
    "print(n_classes)\n",
    "steps = 0\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(batch_size=32, cuda=False, dropout=0.45, clip=0.35, epochs=30, levels=4, log_interval=1, lr=0.0001, optim='Adam', seed=1111, seq_len=204)\n",
      "32\n",
      "32\n",
      "16\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=sample, hidden_size=sample,num_layers=4, batch_first=True) #lstm\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.randn(4, batch_size , sample).cpu()\n",
    "        c = torch.randn(4, batch_size , sample).cpu()\n",
    "        hidden = (h, c)\n",
    "\n",
    "        out1, hidden = self.lstm(x, hidden)\n",
    "        out3 =  self.relu(self.dropout(out1))\n",
    "\n",
    "        return out3\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm= Net()\n",
    "        self.length=204\n",
    "        self.linear = nn.Linear(self.length*4, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x=inputs.permute(0,2,1)\n",
    "        out = self.lstm(x)  \n",
    "        y2 = F.adaptive_avg_pool2d(out, [self.length,4])\n",
    "        y3 = y2.view(y2.size(0), -1)\n",
    "        o = self.linear(y3)\n",
    "        out = F.log_softmax(o, dim=1).contiguous()\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = LSTM(n_classes)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "summary(model, (batch_size, sample, 204),depth=3)\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.shape)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('parameters_count:',count_parameters(model))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 32])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([16, 816])\n",
      "torch.Size([16])\n",
      "parameters_count: 46864\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import torchmetrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "loss_test = []\n",
    "accuracy = []\n",
    "fm = []\n",
    "atts= []\n",
    "acc_valid = []\n",
    "\n",
    "def train(ep):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (bands, target) in enumerate(train_loader):\n",
    " #       if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "        bands, target = Variable(bands), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(bands)\n",
    "        loss = F.nll_loss(output, target.long())\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            los = train_loss.item()/args.log_interval\n",
    "            losses.append(los)\n",
    "            train_loss = 0\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    test_acc = torchmetrics.Accuracy()\n",
    "    test_fm = torchmetrics.ConfusionMatrix(threshold=1./16,num_classes=16)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bands, target in loader:\n",
    "#            if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "#            with torch.no_grad():\n",
    "            bands, target = Variable(bands), Variable(target)\n",
    "            pred = model(bands)\n",
    "            test_loss += F.nll_loss(pred, target.long()).item()\n",
    "            correct += (pred.argmax(1) == target.long()).type(torch.float).sum().item()\n",
    "            test_acc(pred.argmax(1), target.long())\n",
    "            test_fm(pred.argmax(1), target.long())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    total_acc = test_acc.compute()\n",
    "    total_acc = torch.mean(total_acc)\n",
    "    accuracy.append(total_acc)\n",
    "\n",
    "    total_fm = test_fm.compute()\n",
    "    fm.append(total_fm)\n",
    "\n",
    "       \n",
    "    print(f\"Accuracy: {(100 * correct):>0.1f}%, \"f\"Avg loss: {test_loss:>8f}, \"f\"torch metrics acc: {total_acc}%\")\n",
    "\n",
    "    test_fm.reset()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "def valid(loader):\n",
    "    model.eval()\n",
    "    test_acc = torchmetrics.Accuracy()\n",
    "    test_fm = torchmetrics.ConfusionMatrix(threshold=1./16,num_classes=16)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bands, target in loader:\n",
    "#            if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "            with torch.no_grad():\n",
    "                bands, target = Variable(bands), Variable(target)\n",
    "            pred = model(bands)\n",
    "            test_loss += F.nll_loss(pred, target.long()).item()\n",
    "            correct += (pred.argmax(1) == target.long()).type(torch.float).sum().item()\n",
    "            test_acc(pred.argmax(1), target.long())\n",
    "            test_fm(pred.argmax(1), target.long())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    total_acc = test_acc.compute()\n",
    "    acc_valid.append(total_acc)\n",
    "\n",
    "    total_fm = test_fm.compute()\n",
    "    fm.append(total_fm)\n",
    "       \n",
    "    print(f\"Valid Accuracy: {(100 * correct):>0.1f}%, \"f\"Avg loss: {test_loss:>8f}, \"f\"torch metrics acc: {total_acc}%\")\n",
    "\n",
    "    test_acc.reset()\n",
    "    test_fm.reset()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(epoch)\n",
    "        if epoch%5==0:\n",
    "            print(\"Epoch: \",epoch)\n",
    "            print(\"TEST:\") \n",
    "            test(test_loader)\n",
    "#            print(\"VALID:\")\n",
    "#            valid(valid_loader)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  5\n",
      "TEST:\n",
      "Accuracy: 50.2%, Avg loss: 1.046885, torch metrics acc: 0.5018749833106995%\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "TEST:\n",
      "Accuracy: 54.9%, Avg loss: 0.974251, torch metrics acc: 0.5487499833106995%\n",
      "\n",
      "\n",
      "Epoch:  15\n",
      "TEST:\n",
      "Accuracy: 56.6%, Avg loss: 0.963842, torch metrics acc: 0.5662500262260437%\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "TEST:\n",
      "Accuracy: 57.0%, Avg loss: 0.960706, torch metrics acc: 0.5699999928474426%\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "TEST:\n",
      "Accuracy: 57.4%, Avg loss: 0.954610, torch metrics acc: 0.5737500190734863%\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "TEST:\n",
      "Accuracy: 56.9%, Avg loss: 0.957980, torch metrics acc: 0.5693749785423279%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.6 64-bit ('cpu': conda)"
  },
  "interpreter": {
   "hash": "7731885e42f3b5ef5e7f85b92323b7ecd1cf85f8d8a102a156b6474dcda1e534"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}