{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "indice1=300\n",
    "sample=64\n",
    "cl=7\n",
    "\n",
    "train = np.load(\"train.npy\")\n",
    "train = torch.from_numpy(np.stack(train, 0))\n",
    "test = np.load(\"test.npy\")\n",
    "test = torch.from_numpy(np.stack(test, 0))\n",
    "valid = np.load(\"valid.npy\")\n",
    "valid = torch.from_numpy(np.stack(valid, 0))\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "label=np.arange(0,cl,1)\n",
    "label = label.astype(np.float32)\n",
    "train_label = torch.from_numpy(np.tile(label, indice1)) \n",
    "test_label  = torch.from_numpy(np.tile(label, 100))\n",
    "valid_label = torch.from_numpy(np.tile(label, 2))\n",
    "\n",
    "print(train_label.dtype)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "print(valid_label.shape)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_dataset = TensorDataset(train, train_label)\n",
    "valid_dataset = TensorDataset(valid, valid_label)\n",
    "test_dataset = TensorDataset(test, test_label)\n",
    "batch=cl*2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch,            \n",
    "    shuffle= True,          \n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = batch,           \n",
    "    shuffle= True,          \n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch,     \n",
    "    shuffle= True,            \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2100, 64, 204])\n",
      "torch.Size([700, 64, 204])\n",
      "torch.Size([14, 64, 204])\n",
      "torch.float32\n",
      "torch.Size([2100])\n",
      "torch.Size([700])\n",
      "torch.Size([14])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Sequence Modeling')\n",
    "parser.add_argument('--batch_size', type=int, default=cl*2, metavar='N',\n",
    "                    help='batch size (default: 128)')\n",
    "parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                    help='Disable cuda for training')\n",
    "parser.add_argument('--dropout', type=float, default=0.45,\n",
    "                    help='dropout applied to layers (default: 0.05)')\n",
    "parser.add_argument('--clip', type=float, default=0.35,\n",
    "                    help='gradient clip, -1 means no clip (default: -1)')\n",
    "parser.add_argument('--epochs', type=int, default=30,\n",
    "                    help='upper epoch limit (default: 20)')\n",
    "parser.add_argument('--levels', type=int, default=4,\n",
    "                    help='# of levels (default: 8)')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                    help='report interval (default: 100')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='initial learning rate (default: 2e-3)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed (default: 1111)')\n",
    "parser.add_argument('--seq_len', type=int, default=204)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "print(args)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "batch_size = args.batch_size  \n",
    "print(batch_size)\n",
    "epochs = args.epochs \n",
    "input_channels = sample\n",
    "print(input_channels)\n",
    "channel_sizes = [input_channels*2] * args.levels   \n",
    "n_classes = cl\n",
    "print(n_classes)\n",
    "steps = 0\n",
    "\n",
    "\n",
    "import math\n",
    "from einops import rearrange\n",
    "from torch.nn import init\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, p=32, d_model=204, d_k=17*2, h=3, k=5, dropout=.1):\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_o = nn.Linear(d_model, h * d_k)\n",
    "\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, attention_mask=None, attention_weights=None):\n",
    "        b, c, l = x.shape\n",
    "        q = self.fc_q(x).view(b, c, self.h, self.d_k).permute(0, 2, 1, 3) \n",
    "        k = self.fc_k(x).view(b, c, self.h, self.d_k).permute(0, 2, 3, 1) \n",
    "        v = self.fc_v(x).view(b, c, self.h, self.d_v).permute(0, 2, 1, 3) \n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  \n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "\n",
    "        att = torch.softmax(att, dim=1)\n",
    "        att = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b, c, self.h * self.d_v) # (b_s, nq, h*d_v)\n",
    "        att = self.fc_o(att)\n",
    "        att = att+x  \n",
    "        return att\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size,p,l):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.attention = Attention(p,l,l//3)\n",
    "\n",
    "        self.length=l\n",
    "        self.linear = nn.Linear(l*4, output_size)\n",
    "\n",
    "        self.l1 = nn.Linear(204, l) \n",
    "        self.relu1=nn.ReLU()\n",
    "        self.l2=nn.Linear(204, l)      \n",
    "        self.ln = nn.LayerNorm([p,l],elementwise_affine = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x)\n",
    "        out = self.relu1(self.l1(att))\n",
    "        out = self.dropout(self.ln(self.l2(out)+att))\n",
    "\n",
    "        y2 = F.adaptive_avg_pool2d(out.permute(0, 2, 1), [self.length,4])\n",
    "        y3 = y2.view(y2.size(0), -1)\n",
    "        o = self.linear(y3)\n",
    "\n",
    "        o = F.log_softmax(o, dim=1).contiguous()\n",
    "\n",
    "        return o"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(batch_size=14, cuda=False, dropout=0.45, clip=0.35, epochs=30, levels=4, log_interval=1, lr=0.0001, optim='Adam', seed=1111, seq_len=204)\n",
      "14\n",
      "64\n",
      "7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = Transformer(input_channels, n_classes, (64//4)*4, 51*4)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "summary(model, input_size=[(input_channels, 204)], batch_size=batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [14, 64, 204]          41,820\n",
      "            Linear-2              [14, 64, 204]          41,820\n",
      "            Linear-3              [14, 64, 204]          41,820\n",
      "            Linear-4              [14, 64, 204]          41,820\n",
      "     MUSEAttention-5              [14, 64, 204]               0\n",
      "            Linear-6              [14, 64, 204]          41,820\n",
      "              ReLU-7              [14, 64, 204]               0\n",
      "            Linear-8              [14, 64, 204]          41,820\n",
      "         LayerNorm-9              [14, 64, 204]          26,112\n",
      "          Dropout-10              [14, 64, 204]               0\n",
      "           Linear-11                    [14, 7]           5,719\n",
      "================================================================\n",
      "Total params: 282,751\n",
      "Trainable params: 282,751\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.70\n",
      "Forward/backward pass size (MB): 13.95\n",
      "Params size (MB): 1.08\n",
      "Estimated Total Size (MB): 15.72\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import torchmetrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "loss_test = []\n",
    "accuracy = []\n",
    "fm = []\n",
    "atts= []\n",
    "acc_valid = []\n",
    "\n",
    "\n",
    "def train(ep):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (bands, target) in enumerate(train_loader):\n",
    " #       if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "        bands, target = Variable(bands), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(bands)\n",
    "        loss = F.nll_loss(output, target.long())\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            los = train_loss.item()/args.log_interval\n",
    "            losses.append(los)\n",
    "            train_loss = 0\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    test_acc = torchmetrics.Accuracy()\n",
    "    test_fm = torchmetrics.ConfusionMatrix(threshold=1./16,num_classes=16)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bands, target in loader:\n",
    "#            if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "            with torch.no_grad():\n",
    "                bands, target = Variable(bands), Variable(target)\n",
    "            pred = model(bands)\n",
    "            test_loss += F.nll_loss(pred, target.long()).item()\n",
    "            correct += (pred.argmax(1) == target.long()).type(torch.float).sum().item()\n",
    "            test_acc(pred.argmax(1), target.long())\n",
    "            test_fm(pred.argmax(1), target.long())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    total_acc = test_acc.compute()\n",
    "    total_acc = torch.mean(total_acc)\n",
    "    accuracy.append(total_acc)\n",
    "\n",
    "    total_fm = test_fm.compute()\n",
    "    fm.append(total_fm)\n",
    "\n",
    "       \n",
    "    print(f\"Accuracy: {(100 * correct):>0.1f}%, \"f\"Avg loss: {test_loss:>8f}, \"f\"torch metrics acc: {total_acc}%\")\n",
    "\n",
    "    test_fm.reset()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "def valid(loader):\n",
    "    model.eval()\n",
    "    test_acc = torchmetrics.Accuracy()\n",
    "    test_fm = torchmetrics.ConfusionMatrix(threshold=1./16,num_classes=16)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bands, target in loader:\n",
    "#            if args.cuda: bands, target = bands.cuda(), target.cuda()\n",
    "            with torch.no_grad():\n",
    "                bands, target = Variable(bands), Variable(target)\n",
    "            pred = model(bands)\n",
    "            test_loss += F.nll_loss(pred, target.long()).item()\n",
    "            correct += (pred.argmax(1) == target.long()).type(torch.float).sum().item()\n",
    "            test_acc(pred.argmax(1), target.long())\n",
    "            test_fm(pred.argmax(1), target.long())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    total_acc = test_acc.compute()\n",
    "    acc_valid.append(total_acc)\n",
    "\n",
    "    total_fm = test_fm.compute()\n",
    "    fm.append(total_fm)\n",
    "       \n",
    "    print(f\"Valid Accuracy: {(100 * correct):>0.1f}%, \"f\"Avg loss: {test_loss:>8f}, \"f\"torch metrics acc: {total_acc}%\")\n",
    "\n",
    "    test_acc.reset()\n",
    "    test_fm.reset()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(epoch)\n",
    "        if epoch > 15:\n",
    "            print(\"Epoch: \",epoch)\n",
    "            print(\"TEST:\") \n",
    "            test(test_loader)\n",
    "#            print(\"VALID:\")\n",
    "#            valid(valid_loader)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  16\n",
      "TEST:\n",
      "Accuracy: 91.3%, Avg loss: 0.285476, torch metrics acc: 0.9128571152687073%\n",
      "\n",
      "\n",
      "Epoch:  17\n",
      "TEST:\n",
      "Accuracy: 90.5%, Avg loss: 0.294173, torch metrics acc: 0.9049999713897705%\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "TEST:\n",
      "Accuracy: 91.6%, Avg loss: 0.269252, torch metrics acc: 0.9164285659790039%\n",
      "\n",
      "\n",
      "Epoch:  19\n",
      "TEST:\n",
      "Accuracy: 91.1%, Avg loss: 0.270171, torch metrics acc: 0.9114285707473755%\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "TEST:\n",
      "Accuracy: 91.5%, Avg loss: 0.270339, torch metrics acc: 0.9150000214576721%\n",
      "\n",
      "\n",
      "Epoch:  21\n",
      "TEST:\n",
      "Accuracy: 91.7%, Avg loss: 0.263640, torch metrics acc: 0.9171428680419922%\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "TEST:\n",
      "Accuracy: 92.2%, Avg loss: 0.260296, torch metrics acc: 0.9221428632736206%\n",
      "\n",
      "\n",
      "Epoch:  23\n",
      "TEST:\n",
      "Accuracy: 92.0%, Avg loss: 0.262711, torch metrics acc: 0.9200000166893005%\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "TEST:\n",
      "Accuracy: 92.0%, Avg loss: 0.259574, torch metrics acc: 0.9200000166893005%\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "TEST:\n",
      "Accuracy: 92.2%, Avg loss: 0.259404, torch metrics acc: 0.9221428632736206%\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "TEST:\n",
      "Accuracy: 92.3%, Avg loss: 0.259764, torch metrics acc: 0.9228571653366089%\n",
      "\n",
      "\n",
      "Epoch:  27\n",
      "TEST:\n",
      "Accuracy: 92.1%, Avg loss: 0.259620, torch metrics acc: 0.920714259147644%\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "TEST:\n",
      "Accuracy: 92.4%, Avg loss: 0.258461, torch metrics acc: 0.9235714077949524%\n",
      "\n",
      "\n",
      "Epoch:  29\n",
      "TEST:\n",
      "Accuracy: 92.4%, Avg loss: 0.258012, torch metrics acc: 0.9242857098579407%\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "TEST:\n",
      "Accuracy: 92.4%, Avg loss: 0.258243, torch metrics acc: 0.9235714077949524%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.6 64-bit ('cpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40517ded0b5017462b995be888d51a2e84ffa1c8a6e4cf94c222e6054af00f88"
   }
  },
  "interpreter": {
   "hash": "7731885e42f3b5ef5e7f85b92323b7ecd1cf85f8d8a102a156b6474dcda1e534"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
